NAME: Jack Zhang
EMAIL: dvorjackz@ucla.edu
ID: 004993345
SLIPDAYS: 1

QUESTION 2.3.1
Most of the cycles are spent executing list operations. For the 2-thread tests, some of the time goes toward lock operatiosn and spinning as well, but in general, most time is still spent on list operations, as can be verified by the graph.

The most expensive parts of the code are locking, spinning, and each of the four list operations.

For the high-thread spin lock tests, most of the time is spent spinning, since each thread that does not obtain a lock keeps spinning until it receives the key.

For the high-thread mutex lock tests, it is not as definitive, but I believe most of the time is spent on list operations if the list if large, since the lookup, length, and insertion functions are O(n). Also, mutex threads do not keep running and hogging CPU when it does not obtain a lock and simply sleeps, freeing up CPU for the main list functions, unlike the spin threads. Thus, they are more efficient. If the list size is small though, most of the time is spent on locking operations.

QUESTION 2.3.2
As we can see from the profile produced by pprof, the bulk of the execution time is spent on spinning and waiting for a key to access the insertion and deletion critical sections. There is very little time being spent on the list operations themselves.

The spinning becomes very expensive as the nunmber of threads increases, because if threads without locks are spinning, the thread that actually needs to finish the list operations is not running. As there are more threads, this effect becomes amplified and the program becomes even slower.

QUESTION 2.3.3
The average wait-for-lock time increases dramatically because there are more threads competing for the lock, and consequently more time-costly mutex checking for whether the lock is available by each thread. 

Average time per operation increases becasue of the increasing number of context switches associated with more threads. 

Completion time is equal to wait time plus the time for everything else: list operations, etc. The non-wait time is much less time costly and grows slower than lock-wait time, so overall completion time will grow more slowly than wait time.

QUESTION 2.3.4
The performance increases as the number of sublists increases. This is because dividing up the list into groups with their own locks drastically lowers contention among threads. Also, since each sublist is smaller to work with, insertion, length, and lookup times will decreases since there are less nodes to traverse.

The throughput should keep increasing, but after a certain point where contention is very unlikely to happen, the throughput increase should level off.

No. One reason is that the smaller length of the sublists result in faster list operations. Thus, critical sections are less time-intensive.

TARBALL CONTENTS:

SortedList.h - a header file containing interfaces for linked list operations.
SortedList.c - the source for a C source module that implements insert, delete, lookup, and length methods for a sorted doubly linked list.
Makefile - to build the deliverable programs, output, graphs, and tarball. Targets include:
	 default ... the lab2_list executable (compiling with the -Wall and -Wextra options).
	 tests ... run all specified test cases to generate CSV results
	 profile ... run tests with profiling tools to generate an execution profiling report
	 graphs ... use gnuplot to generate the required graphs
	 dist ... create the deliverable tarball
	 clean ... delete all programs and output generated by the Makefile
lab2b_list.csv - containing results for all test runs.
profile.out - execution profiling report showing where time was spent in the un-partitioned spin-lock implementation.
lab2b_1.png ... throughput vs. number of threads for mutex and spin-lock synchronized list operations.
lab2b_2.png ... mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
lab2b_3.png ... successful iterations vs. threads for each synchronization method.
lab2b_4.png ... throughput vs. number of threads for mutex synchronized partitioned lists.
lab2b_5.png ... throughput vs. number of threads for spin-lock-synchronized partitioned lists.